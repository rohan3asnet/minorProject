{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3297b3-cc38-41eb-8dcf-037793bd43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27788444-203f-4edf-81a7-ea8a6a31f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#for defining convolutional layer, pooling layers, fully connected layer,etc\n",
    "import torch.nn as nn \n",
    "#for implementing optimizing algorithm like adam\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "#for image preprocessing and agumentation\n",
    "from torchvision.transforms import v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2998e415-3fdb-4aee-b54c-9f7a94d3d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e3ed99-db65-455a-beb9-618988929157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9f588e-5e9d-4984-90db-99d4b05ec684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fft2 perform 2d fast fourier transform on a 2d array\n",
    "#fft2 transfrom image from spatial domain to frequency domain\n",
    "#ifft2 does the inverse\n",
    "from scipy.fft import fft2, ifft2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eeda0be-dd5f-4106-a0f5-856737e43609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3363784d-b733-442b-8f14-aa8b371d23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a custom dataset\n",
    "class pneumoniaDataset(Dataset):\n",
    "    def __init__(self,image_path,device='cpu',transform=None):\n",
    "        self.image_path = image_path\n",
    "        self.device = device\n",
    "        self.transform=transform\n",
    "        self.data = []\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        for label, folder_name in enumerate(os.listdir(self.image_path)):\n",
    "            folder_path = os.path.join(self.image_path, folder_name)\n",
    "            if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "                print(f\"Processing folder: {folder_path},{folder_name} as label {label}\")\n",
    "                for image_name in os.listdir(folder_path):\n",
    "                    if image_name.endswith(('.png', '.jpg', '.jpeg')):  # Ensure valid image files\n",
    "                        image_path = os.path.join(folder_path, image_name)\n",
    "                        #print(f\"Adding image: {image_path}\")\n",
    "                        self.data.append((image_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "            return len(self.data)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        image_path, label = self.data[index]\n",
    "        image = Image.open(image_path).convert('RGB')  # Load image and convert to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations\n",
    "        label = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef532a9e-3102-4fc8-8e37-d93e43bb6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.Resize((227, 227)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95a9676-d960-4a71-8198-308bce878ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since i was struglling with automatically appearing of .DS_store in my folder\n",
    "#which lead to unwanted classifying as class and  labeling it as 0 \n",
    "# Remove .DS_Store files from the folder and subfolders\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    if '.DS_Store' in files:\n",
    "        os.remove(os.path.join(root, '.DS_Store'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4f1f55-c34d-4cc2-bef9-f07586155db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: data/train_set/bacterial,bacterial as label 0\n",
      "Processing folder: data/train_set/normal,normal as label 1\n",
      "Processing folder: data/train_set/viral,viral as label 2\n",
      "Processing folder: data/val_set/bacterial,bacterial as label 0\n",
      "Processing folder: data/val_set/normal,normal as label 1\n",
      "Processing folder: data/val_set/viral,viral as label 2\n",
      "Processing folder: data/test_set/bacterial,bacterial as label 0\n",
      "Processing folder: data/test_set/normal,normal as label 1\n",
      "Processing folder: data/test_set/viral,viral as label 2\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset\n",
    "train_path=os.path.join('data/train_set')\n",
    "validation_path=os.path.join('data/val_set')\n",
    "test_path=os.path.join('data/test_set')\n",
    "train_dataset = pneumoniaDataset(image_path=train_path, transform=transform)\n",
    "validation_dataset = pneumoniaDataset(image_path=validation_path, transform=transform)\n",
    "test_dataset = pneumoniaDataset(image_path=test_path, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8604ea8-6429-4260-996c-c40d967c7f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7134"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdcb0224-2db8-45af-807f-8422beb7a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "793\n",
      "torch.Size([3, 227, 227])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# print(len(validation_dataset))\n",
    "# image, label = train_dataset[1] \n",
    "# print(image.shape)         \n",
    "# print(label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45004835-d1b2-4138-bf8f-bc237307aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "validation_dataloader=DataLoader(validation_dataset,batch_size=32,shuffle=False)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2df1e7b5-88c1-4280-9db9-cc0d49a9e6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 227, 227])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "#Checking the first batch from the train_loader\n",
    "for images, labels in train_dataloader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)  \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d939df-2d47-4873-b823-4cd9e34edf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self,input_channels,number_of_classes):\n",
    "        super(AlexNet, self).__init__() \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 96, kernel_size=11, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4069)\n",
    "        self.fc2 = nn.Linear(4069, 4069)\n",
    "        self.fc3 = nn.Linear(4069, number_of_classes)\n",
    "\n",
    "        # Other\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.norm = nn.LocalResponseNorm(size=5, k=2)\n",
    "        self.droput = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.norm(self.relu(self.conv1(x))))  # (B, 96, 27, 27)\n",
    "        x = self.maxpool(self.norm(self.relu(self.conv2(x))))  # (B, 256, 13, 13)\n",
    "        x = self.relu(self.conv3(x))                           # (B, 384, 13, 13)\n",
    "        x = self.relu(self.conv4(x))                           # (B, 384, 13, 13)\n",
    "        x = self.maxpool(self.relu(self.conv5(x)))             # (B, 256, 6, 6)\n",
    "        x = self.flatten(x)                                    # (B, 9216)\n",
    "        x = self.droput(self.relu(self.fc1(x)))                # (B, 4096)\n",
    "        x = self.droput(self.relu(self.fc2(x)))                # (B, 4096)\n",
    "        x = self.fc3(x)                                        # (B, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59ef94a9-86d1-487f-9180-227271fe8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AlexNet(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d6d848-9f4d-4fa4-89db-5c70cf4045c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs=10\n",
    "learning_rate=0.001\n",
    "\n",
    "#loss function\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer=optim.Adam(model.parameters(),learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13080d52-11d7-4311-bcbb-d0a2965bd3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:32<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.5391, Val Loss: 1.0645, Val Acc: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:47<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 1.6237, Val Loss: 1.0651, Val Acc: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:51<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 1.6207, Val Loss: 1.0649, Val Acc: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:54<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 1.5396, Val Loss: 1.0640, Val Acc: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:53<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 1.4998, Val Loss: 1.0642, Val Acc: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:51<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 1.5556, Val Loss: 1.0642, Val Acc: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:47<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 1.5020, Val Loss: 1.0641, Val Acc: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:50<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 1.5837, Val Loss: 1.0644, Val Acc: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:50<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 1.5723, Val Loss: 1.0643, Val Acc: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 223/223 [05:54<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 1.5780, Val Loss: 1.0642, Val Acc: 41.24%\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_dataloader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    model.train()\n",
    "    for images, labels in tqdm(train_dataloader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images,labels in validation_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs,labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # For accuracy calculation (modify if needed for your task)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total +=labels.size(0)\n",
    "            val_correct += (predicted ==labels).sum().item()\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    avg_val_loss = val_loss / len(validation_dataloader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{number_of_epochs}], '\n",
    "          f'Train Loss: {loss.item():.4f}, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}, '\n",
    "          f'Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "719fe777-fbb5-4dc6-9978-eb53cebbd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './alexNet_MP.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302eb66d-a291-4567-a3e8-61e75eb16822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
