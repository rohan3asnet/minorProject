{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d788330-2b3f-461c-8314-a6658157c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cd6d99-3e63-4dd9-8881-8412153fe803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#for defining convolutional layer, pooling layers, fully connected layer,etc\n",
    "import torch.nn as nn \n",
    "#for implementing optimizing algorithm like adam\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "#for image preprocessing and agumentation\n",
    "from torchvision.transforms import v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c057f6-a29c-4cd2-a60a-3025906cfa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fft as fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff74d9a-b2fd-47b2-a955-2e8c3dfc6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e6c79a-1512-408e-83b5-2eb10805c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b66dc92-ad09-46cb-97e3-b357f8535ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pneumoniaDataset(Dataset):\n",
    "    def __init__(self, image_path, device):\n",
    "        self.image_path = image_path\n",
    "        self.device = device\n",
    "        self.data = []\n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize((227,227)),\n",
    "            v2.Normalize(mean=[0.5,], std=[0.5,]),\n",
    "            v2.Grayscale(num_output_channels=3)\n",
    "        ])\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        for idx, label in enumerate(os.listdir(os.path.join(self.image_path))):\n",
    "            print(os.path.join(self.image_path, label))\n",
    "            for img_file in tqdm(os.listdir(os.path.join(self.image_path, label))):\n",
    "                img = cv2.imread(os.path.join(self.image_path, label, img_file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                img = self.transform(img)\n",
    "\n",
    "                idx = torch.tensor([idx])\n",
    "                self.data.append((img.to(self.device), idx.to(self.device)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img,label= self.data[idx]\n",
    "        label=label.squeeze()\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27e36e4-cfa0-4a94-b691-629af4a2a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since i was struglling with automatically appearing of .DS_store in my folder\n",
    "# which lead to unwanted classifying as class and  labeling it as 0 \n",
    "# Remove .DS_Store files from the folder and subfolders\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    if '.DS_Store' in files:\n",
    "        os.remove(os.path.join(root, '.DS_Store'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42e952b-70b3-412c-906a-bf5eb243f3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_set/bacterial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2700/2700 [00:10<00:00, 251.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_set/normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2943/2943 [00:23<00:00, 124.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_set/viral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1491/1491 [00:07<00:00, 203.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/val_set/bacterial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 301/301 [00:01<00:00, 204.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/val_set/normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 327/327 [00:04<00:00, 67.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/val_set/viral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 165/165 [00:00<00:00, 169.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset\n",
    "train_path=os.path.join('data/train_set')\n",
    "validation_path=os.path.join('data/val_set')\n",
    "# test_path=os.path.join('data/test_set')\n",
    "train_dataset = pneumoniaDataset(image_path=train_path, device=device)\n",
    "validation_dataset = pneumoniaDataset(image_path=validation_path, device=device)\n",
    "# test_dataset = pneumoniaDataset(image_path=test_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f6b302d-38c9-4b48-bfb1-1f06532b5e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7134, 793)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "099eba50-6812-443c-9a7d-158d802e0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "batch_size = 8\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "validation_dataloader=DataLoader(validation_dataset,batch_size=batch_size,shuffle=False)\n",
    "# test_dataloader=DataLoader(test_dataset,batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66de5850-3dd2-44b7-9ac2-da6d90717ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(validation_dataset))\n",
    "# image, label = train_dataset[6000] \n",
    "# print(image.shape)         \n",
    "# print(label)  \n",
    "# print(label.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf69c7e8-c7ea-4fb1-9c33-0d03339e8356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTConvNet(nn.Module):\n",
    "    def __init__(self, conv_layer, fft_filter=None):\n",
    "        super().__init__()\n",
    "        self.conv_layer = conv_layer  # Original Conv2d layer\n",
    "        self.fft_filter = fft_filter\n",
    "\n",
    "    def fft_filter_def(self, fft_x, height, width):\n",
    "        cht, cwt = height // 2, width // 2\n",
    "        mask_radius = 30\n",
    "\n",
    "        # Create a meshgrid for the mask\n",
    "        fy, fx = torch.meshgrid(\n",
    "            torch.arange(0, height, device=fft_x.device),\n",
    "            torch.arange(0, width, device=fft_x.device),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        mask_area = torch.sqrt((fx - cwt) ** 2 + (fy - cht) ** 2)\n",
    "\n",
    "        # Create the mask based on the filter type\n",
    "        if self.fft_filter == 'high':\n",
    "            mask = (mask_area > mask_radius).float()\n",
    "        else:\n",
    "            mask = (mask_area <= mask_radius).float()\n",
    "\n",
    "        # Apply the mask to the FFT of the input\n",
    "        filtered_fft = fft_x * mask\n",
    "        return filtered_fft\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, in_channels, height, width = x.size()\n",
    "        out_channels = self.conv_layer.out_channels  # Number of output channels\n",
    "\n",
    "        # Apply FFT on input image\n",
    "        fft_x = fft.fft2(x)  # Shape: [batch_size, in_channels, height, width]\n",
    "        fft_x = fft.fftshift(fft_x)\n",
    "\n",
    "        # Apply FFT on the convolutional kernel\n",
    "        kernel_fft = fft.fft2(self.conv_layer.weight, s=(height, width))  # Shape: [out_channels, in_channels, height, width]\n",
    "        kernel_fft = fft.fftshift(kernel_fft)\n",
    "\n",
    "        # Apply FFT filter (low-pass or high-pass)\n",
    "        if self.fft_filter is not None:\n",
    "            fft_x = self.fft_filter_def(fft_x, height, width)\n",
    "\n",
    "        # Perform element-wise complex multiplication\n",
    "        fft_output = fft_x.unsqueeze(1) * kernel_fft.unsqueeze(0)  # Broadcast multiplication\n",
    "        fft_output = torch.sum(fft_output, dim=2)  # Sum over input channels\n",
    "\n",
    "        # Apply inverse FFT\n",
    "        fft_output = fft.ifftshift(fft_output, dim=(-2, -1))\n",
    "        spatial_output = fft.ifft2(fft_output, dim=(-2, -1)).real  # Shape: [batch_size, out_channels, height, width]\n",
    "\n",
    "        # Add bias (if applicable)\n",
    "        if self.conv_layer.bias is not None:\n",
    "            spatial_output += self.conv_layer.bias.view(1, -1, 1, 1)\n",
    "\n",
    "        # Debug: Print shapes\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        print(f\"Output shape: {spatial_output.shape}\")\n",
    "\n",
    "        # Return output\n",
    "        return spatial_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e51d733f-5edf-4377-a4b7-a1443dbb78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self,input_channels,number_of_classes):\n",
    "        super(AlexNet, self).__init__() \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 96, kernel_size=11, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(186624, 4069)\n",
    "        self.fc2 = nn.Linear(4069, 4069)\n",
    "        self.fc3 = nn.Linear(4069, number_of_classes)\n",
    "\n",
    "        # Other\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.norm = nn.LocalResponseNorm(size=5, k=2)\n",
    "        self.droput = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.norm(self.relu(self.conv1(x))))  # (B, 96, 27, 27)\n",
    "        x = self.maxpool(self.norm(self.relu(self.conv2(x))))  # (B, 256, 13, 13)\n",
    "        x = self.relu(self.conv3(x))                           # (B, 384, 13, 13)\n",
    "        x = self.relu(self.conv4(x))                           # (B, 384, 13, 13)\n",
    "        x = self.maxpool(self.relu(self.conv5(x)))             # (B, 256, 6, 6)\n",
    "        x = self.flatten(x)                                    # (B, 9216)\n",
    "        x = self.droput(self.relu(self.fc1(x)))                # (B, 4096)\n",
    "        x = self.droput(self.relu(self.fc2(x)))                # (B, 4096)\n",
    "        x = self.logsoftmax(self.fc3(x))                                       # (B, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9329e0-93b3-4278-a418-da2790e5af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AlexNet(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89af033b-d10b-4a2f-a436-1ec7d4469d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_children():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        if module.kernel_size[0] > 3:\n",
    "            fft_cnn = FFTConvNet(module, 'low')\n",
    "            setattr(model, name, fft_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be9325-c23b-4e6e-9500-d87cd70d7298",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46972a1a-cfaa-4979-a1ff-8882816f9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "#loss function\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer=optim.Adam(model.parameters(),learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db18d45f-f106-4d3e-9f89-1a66683f1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, loss_fn, optimizer, epochs):\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, labels in tqdm(train_dl):\n",
    "            # print(\"Data loaded\")\n",
    "\n",
    "            # Move data to GPU\n",
    "            labels = labels.squeeze().long()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # print(f\"Data moved to {device}\")\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # print(\"Forward pass completed\")\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # print(\"Loss computed\")\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(\"Backward pass completed\")\n",
    "\n",
    "            # Compute metrics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        print(f\"Epoch Loss: {epoch_loss:.4f}, Epoch Accuracy: {epoch_acc:.4f}\")\n",
    "    print('Training Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0db16-32d1-4b10-adca-767119e28f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 3, 227, 227])\n",
      "Output shape: torch.Size([8, 96, 227, 227])\n",
      "Input shape: torch.Size([8, 96, 113, 113])\n",
      "Output shape: torch.Size([8, 256, 113, 113])\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04f2da-ca1c-49c8-9e23-2829cdd65523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_step = len(train_dataloader)\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# for epoch in range(number_of_epochs):\n",
    "#     model.train()\n",
    "#     for images, labels in tqdm(train_dataloader):  \n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         labels= labels.long()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_correct = 0\n",
    "#     val_total = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images,labels in validation_dataloader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             labels= labels.long()\n",
    "            \n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs,labels)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             val_total +=labels.size(0)\n",
    "#             val_correct += (predicted ==labels).sum().item()\n",
    "\n",
    "#     # Calculate validation metrics\n",
    "#     avg_val_loss = val_loss / len(validation_dataloader)\n",
    "#     val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{number_of_epochs}], '\n",
    "#           f'Train Loss: {loss.item():.4f}, '\n",
    "#           f'Val Loss: {avg_val_loss:.4f}, '\n",
    "#           f'Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "# print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef07db8-bb4b-4278-8a2c-d230d63babe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './alexNetModel.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
